# 《自然语言处理》大作业2报告

邬心远

519021910604

## 代码实现

代码要求实现word2vec网络中的一次前向、后向更新。其中利用到的公式是计算隐藏层数据：

![2-4](.\pic\2-4.png)

计算loss:

![2-5](.\pic\2-5.png)

更新隐藏结点到输出层参数：

![2-6](.\pic\2-6.png)

更新输入到隐藏结点参数：

![2-7](.\pic\2-7.png)

在代码中的实现如下：

```python
    def train_one_step(self, context_tokens: List[str], target_token: str, learning_rate: float) -> float:
        # ==== Construct one-hot vectors ====
        inputs = np.array([one_hot(len(self.vocab), self.vocab.token_to_idx(context))
                           for context in context_tokens])  # C*V

        # ==== Forward step ====
        inputs_avg = np.mean(inputs, axis=0)
        h = np.matmul(inputs_avg, self.W1)  # 1*N
        output = np.matmul(h, self.W2)  # 1*V
        output = softmax(output)
        # print('output', output)

        # ==== Calculate loss ====
        target_idx = self.vocab.token_to_idx(target_token)
        loss = - np.log(output[target_idx])

        # ==== Update parameters ====
        e = output - one_hot(len(self.vocab), target_idx)  # V
        self.W1 = self.W1 - np.matmul(inputs_avg.reshape(len(self.vocab), 1),
                                      np.matmul(e, np.transpose(self.W2)).reshape(1, self.vector_dim)*learning_rate)
        self.W2 = self.W2 - learning_rate * np.matmul(h.reshape(self.vector_dim, 1), e.reshape(1, len(self.vocab)))

        return loss
```

## 实验结果

最终三个测试的实验结果如下：

- test1:

  ![2-1](.\pic\2-1.png)

- test2:

  ![2-2](.\pic\2-2.png)

- test3:

  ![2-3](.\pic\2-3.png)

可以看到test1结果中'I','he','she'的相似度较高，test2最终loss为6.92，test3中spearman相关系数为0.397且pearson相关系为0.551。均符合要求。